{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114f3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e73324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " Price        0\n",
      "Age_08_04    0\n",
      "KM           0\n",
      "Fuel_Type    0\n",
      "HP           0\n",
      "Automatic    0\n",
      "cc           0\n",
      "Doors        0\n",
      "Cylinders    0\n",
      "Gears        0\n",
      "Weight       0\n",
      "dtype: int64\n",
      "Features used: ['Age_08_04', 'KM', 'HP', 'Automatic', 'cc', 'Doors', 'Cylinders', 'Gears', 'Weight', 'Fuel_Type_Diesel', 'Fuel_Type_Petrol']\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Preprocessing\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\chall\\\\OneDrive\\\\Desktop\\\\MLR\\\\MLR\\\\ToyotaCorolla - MLR.csv\")\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Encode categorical variable Fuel_Type\n",
    "df = pd.get_dummies(df, columns=[\"Fuel_Type\"], drop_first=True)\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=['Price'])\n",
    "y = df['Price']\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "print(\"Features used:\", X_scaled.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86cdb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             feature        VIF\n",
      "0          Age_08_04   1.920520\n",
      "1                 KM   2.001790\n",
      "2                 HP   2.299766\n",
      "3          Automatic   1.094550\n",
      "4                 cc   1.223892\n",
      "5              Doors   1.217898\n",
      "6          Cylinders        NaN\n",
      "7              Gears   1.117413\n",
      "8             Weight   3.297145\n",
      "9   Fuel_Type_Diesel  11.317251\n",
      "10  Fuel_Type_Petrol   9.702194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chall\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1738: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 1 - self.ssr/self.uncentered_tss\n"
     ]
    }
   ],
   "source": [
    "# 2. Multicollinearity Check\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['feature'] = X.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "vif_df = calculate_vif(X_scaled)\n",
    "print(vif_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42fd17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Building\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model 1: Standard Multiple Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Model 2: Linear Regression with Interaction Terms (Age * KM, HP * Weight)\n",
    "X_int = X_scaled.copy()\n",
    "X_int['Age_KM'] = X_int['Age_08_04'] * X_int['KM']\n",
    "X_int['HP_Weight'] = X_int['HP'] * X_int['Weight']\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_int, y, test_size=0.2, random_state=42)\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(X_train2, y_train2)\n",
    "y_pred_lr2 = lr2.predict(X_test2)\n",
    "\n",
    "# Model 3: Polynomial Features (degree=2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "lr3 = LinearRegression()\n",
    "lr3.fit(X_train3, y_train3)\n",
    "y_pred_lr3 = lr3.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf29cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "  RMSE: 1484.27\n",
      "  MAE: 990.89\n",
      "  R2: 0.835\n",
      "\n",
      "Linear Regression + Interactions:\n",
      "  RMSE: 1368.30\n",
      "  MAE: 891.42\n",
      "  R2: 0.860\n",
      "\n",
      "Polynomial Regression (deg=2):\n",
      "  RMSE: 1786.79\n",
      "  MAE: 908.95\n",
      "  R2: 0.761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Model Evaluation\n",
    "def eval_model(y_test, y_pred, name=\"Model\"):\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{name}:\\n  RMSE: {rmse:.2f}\\n  MAE: {mae:.2f}\\n  R2: {r2:.3f}\\n\")\n",
    "\n",
    "eval_model(y_test, y_pred_lr, \"Linear Regression\")\n",
    "eval_model(y_test2, y_pred_lr2, \"Linear Regression + Interactions\")\n",
    "eval_model(y_test3, y_pred_lr3, \"Polynomial Regression (deg=2)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ce290e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression:\n",
      "  RMSE: 1483.47\n",
      "  MAE: 990.87\n",
      "  R2: 0.835\n",
      "\n",
      "Lasso Regression:\n",
      "  RMSE: 1484.16\n",
      "  MAE: 990.90\n",
      "  R2: 0.835\n",
      "\n",
      "                        Linear        Ridge        Lasso\n",
      "Age_08_04        -2.246662e+03 -2244.540333 -2246.643042\n",
      "KM               -6.085706e+02  -610.243379  -608.624409\n",
      "HP                2.102533e+02   211.552393   210.364888\n",
      "Automatic         3.413608e+01    34.349741    34.078222\n",
      "cc               -1.288507e+01   -12.749655   -12.786570\n",
      "Doors            -5.743684e+01   -56.548223   -57.254803\n",
      "Cylinders         3.410605e-13     0.000000     0.000000\n",
      "Gears             1.039463e+02   104.054836   103.883104\n",
      "Weight            1.362139e+03  1359.382176  1361.705593\n",
      "Fuel_Type_Diesel -2.127088e+01   -21.825934   -21.029790\n",
      "Fuel_Type_Petrol  4.451027e+02   441.206721   444.955639\n"
     ]
    }
   ],
   "source": [
    "# 5. Lasso and Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "eval_model(y_test, y_pred_ridge, \"Ridge Regression\")\n",
    "eval_model(y_test, y_pred_lasso, \"Lasso Regression\")\n",
    "\n",
    "# Compare Coefficients: Standard vs Ridge vs Lasso\n",
    "coef_df = pd.DataFrame({\n",
    "    'Linear': lr.coef_,\n",
    "    'Ridge': ridge.coef_,\n",
    "    'Lasso': lasso.coef_\n",
    "}, index=X_scaled.columns)\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f7eba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with VIF > 10 (potential multicollinearity):\n",
      "            feature        VIF\n",
      "9  Fuel_Type_Diesel  11.317251\n"
     ]
    }
   ],
   "source": [
    "# 6. Addressing Multicollinearity\n",
    "print(\"Features with VIF > 10 (potential multicollinearity):\")\n",
    "print(vif_df[vif_df['VIF'] > 10])\n",
    "# To drop high VIF features and rebuild model:\n",
    "# X_reduced = X_scaled.drop(columns=vif_df[vif_df['VIF'] > 10]['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interview Questions & Answers\n",
    "\n",
    "1. What is Normalization & Standardization and how is it helpful?  \n",
    "\n",
    "- Normalization: Scales feature values into a fixed range, usually [0,1].  \n",
    "  Formula:  \n",
    "  \\[\n",
    "  x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "  \\]  \n",
    "  Useful when features have different ranges or when algorithms rely on distance (e.g., KNN, K-means).  \n",
    "\n",
    "- Standardization: Transforms features to have zero mean and unit variance (standard normal distribution).  \n",
    "  Formula:  \n",
    "  \\[\n",
    "  x' = \\frac{x - \\mu}{\\sigma}\n",
    "  \\]  \n",
    "  Helpful for algorithms sensitive to scale (e.g., Linear Regression, Logistic Regression, PCA).  \n",
    "\n",
    "Why it helps in regression:  \n",
    "- Prevents features with large scales from dominating coefficients.  \n",
    "- Improves numerical stability and interpretability.  \n",
    "- Helps optimization converge faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38da58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation\n",
    "### Interpretation of Coefficients\n",
    "\n",
    "Each coefficient in a multiple linear regression model represents the expected change in the target variable (Price) \n",
    "for a one-unit increase in the predictor variable, holding all other predictors constant.\n",
    "\n",
    "For example:\n",
    "- If the coefficient of `Age` is `-120`, it means that for every additional year of the car's age, the price is expected to decrease by 120 units, assuming other factors remain constant.\n",
    "- If the coefficient of `KM` is `-0.02`, then each additional kilometer driven reduces the car's price by 0.02 units, keeping other features constant.\n",
    "- Positive coefficients (e.g., `HP`) indicate that higher horsepower increases the predicted car price.*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321a067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
